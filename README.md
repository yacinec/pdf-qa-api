# ðŸ§  PDF Question-Answering API with Mistral

This project is a local API that allows you to upload a PDF, automatically index its content using FAISS and SentenceTransformers, and ask semantic questions answered by a local LLM (Mistral 7B Instruct via `llama-cpp-python`).

---

## Features

- Upload and validate PDF files
- Extract and chunk text from PDF
- Create vector index (FAISS + `all-MiniLM-L6-v2`)
- Ask semantic questions using a local Mistral model
- No internet required â€” works fully offline after model download

---

## Project Structure

```
pdf_qa_api/
â”œâ”€â”€ app/
â”‚ â”œâ”€â”€ main.py
â”‚ â”œâ”€â”€ pdf_utils.py
â”‚ â”œâ”€â”€ vector_store.py
â”‚ â”œâ”€â”€ llm_interface.py
â”‚ â””â”€â”€ config.py
â”œâ”€â”€ models/
â”œâ”€â”€ uploads/
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## Quickstart

1. Clone this repo
2. Download a `.gguf` model (e.g. Mistral 7B Instruct) into `models/`
3. Install dependencies:
```bash
pip install -r requirements.txt
```

4. Run the API:
```bash
uvicorn app.main:app --reload
```

5. Open the docs: http://localhost:8000/docs

## Endpoints

- `POST /upload_pdf`: Upload a PDF (max 10MB) â†’ triggers indexing
- `GET /answer?question=...`: Ask a semantic question about the PDF

## Dependencies

- FastAPI
- PyMuPDF
- sentence-transformers
- faiss-cpu
- llama-cpp-python


## Security

- Validates MIME type and size
- Refuses non-PDF files


## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
